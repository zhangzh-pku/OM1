---
title: LLM Models
description: "Using Different LLM Models as the Core LLM"
---

## Emotion Detection with Various LLM Providers

These examples are similar to the `Hello World (Spot)` example, except they use different LLM providers rather than `OpenAI 4o`.

The examples perform emotion detection, interact with you with voice, and provide debug information to a simulator. The code may request access to your camera, this is what it uses to capture your emotions.

### Available Providers

**DeepSeek:**
```bash
uv run src/run.py deepseek
```

**Google Gemini:**
```bash
uv run src/run.py gemini
```

**xAI (Grok):**
```bash
uv run src/run.py grok
```

**Mistral AI:** ðŸ†•
```bash
uv run src/run.py spot_mistral
```

**Meta Llama:** ðŸ†•
```bash
uv run src/run.py spot_llama
```

**Local Deployment:**
```bash
uv run src/run.py spot_local
```

### Configuration

Each provider requires appropriate API keys set in your `.env` file:

```bash
# .env
DEEPSEEK_API_KEY=your-deepseek-key
GOOGLE_API_KEY=your-gemini-key
XAI_API_KEY=your-grok-key
MISTRAL_API_KEY=your-mistral-key
LLAMA_API_KEY=your-llama-key
```

For local deployments, see the configuration examples in `config/spot_local.json5`.

A web-based simulation environment (WebSim) can be accessed at http://localhost:8000.